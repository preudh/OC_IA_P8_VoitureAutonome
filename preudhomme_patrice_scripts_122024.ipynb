{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **1. Importation des bibliothèques**",
   "id": "65d48c413aae83d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:13:34.754433Z",
     "start_time": "2024-12-06T15:13:34.749011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, UpSampling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import cv2\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score, jaccard_score"
   ],
   "id": "b0d019d21b0005de",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:13:40.184971Z",
     "start_time": "2024-12-06T15:13:40.179804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Version TensorFlow :\", tf.__version__)\n",
    "print(\"GPU disponible :\", tf.config.list_physical_devices('GPU'))\n"
   ],
   "id": "5e86fa381447add1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version TensorFlow : 2.10.0\n",
      "GPU disponible : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **2. Préparation des données avec Générateurs de Batches**\n",
    " - Dans cette cellule, nous chargeons les images RGB d'origine ainsi que les masques de segmentation à partir des dossiers train, val, et test qui sont déjà préparés. Les images sont redimensionnées à une taille plus petite (256x256) pour rendre l'entraînement plus efficace, puis normalisées.\n",
    "  - Le jeu de données est composé de 2 dossiers:\n",
    "● leftImg8bit : ce dossier contient les images RGB d’origines. Ce sont les données d’éntrée\n",
    "● gtFine : ce dossier contient les masques de segmentation. Ce sont les données de sortie\n",
    "- En ce qui concerne les données de sortie, il y a plusieurs types de fichiers, mais nous ne nous intéresserons qu’aux\n",
    "fichiers dont le nom se termine par “_labelIds.png”. Ce sont les masques correspondant aux images\n",
    "RGB."
   ],
   "id": "e5676f8005654089"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:31:05.350815Z",
     "start_time": "2024-12-06T15:31:04.562031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Définitions des chemins de fichiers avec chemins relatifs\n",
    "base_dir = os.path.join(os.getcwd(), 'data')\n",
    "image_dir_train = os.path.join(base_dir, 'leftImg8bit/train')\n",
    "mask_dir_train = os.path.join(base_dir, 'gtFine/train')\n",
    "image_dir_val = os.path.join(base_dir, 'leftImg8bit/val')\n",
    "mask_dir_val = os.path.join(base_dir, 'gtFine/val')\n",
    "image_dir_test = os.path.join(base_dir, 'leftImg8bit/test')\n",
    "mask_dir_test = os.path.join(base_dir, 'gtFine/test')\n",
    "\n",
    "# Générateur de données pour charger les images par batch\n",
    "# Générateur de données pour charger les images par batch\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_dir, mask_dir, batch_size=16, img_size=(256, 256)):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.image_list = []\n",
    "\n",
    "        # Parcours récursif des dossiers pour les images\n",
    "        for root, _, files in os.walk(image_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.image_list.append(os.path.join(root, file))\n",
    "\n",
    "        # Parcours récursif des dossiers pour les masques (uniquement `_labelIds.png`)\n",
    "        self.mask_list = []\n",
    "        for root, _, files in os.walk(mask_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('_labelIds.png'):\n",
    "                    self.mask_list.append(os.path.join(root, file))\n",
    "\n",
    "        # S'assurer que les listes sont triées pour que chaque image corresponde au bon masque\n",
    "        self.image_list.sort()\n",
    "        self.mask_list.sort()\n",
    "\n",
    "        self.indexes = np.arange(len(self.image_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        images, masks = [], []\n",
    "\n",
    "        for i in batch_indexes:\n",
    "            img_path = self.image_list[i]\n",
    "            mask_path = self.mask_list[i]\n",
    "            img = image.load_img(img_path, target_size=self.img_size)\n",
    "            img = image.img_to_array(img) / 255.0\n",
    "            mask = image.load_img(mask_path, color_mode=\"grayscale\", target_size=self.img_size)\n",
    "            mask = image.img_to_array(mask) / 255.0\n",
    "\n",
    "            images.append(img)\n",
    "            masks.append(mask)\n",
    "\n",
    "        return np.array(images), np.array(masks)\n",
    "\n",
    "\n",
    "# Création des générateurs de données pour train, validation et test\n",
    "batch_size = 16\n",
    "train_gen = DataGenerator(image_dir_train, mask_dir_train, batch_size=batch_size)\n",
    "val_gen = DataGenerator(image_dir_val, mask_dir_val, batch_size=batch_size)\n",
    "test_gen = DataGenerator(image_dir_test, mask_dir_test, batch_size=batch_size)\n"
   ],
   "id": "8b1bd2d9dd815da3",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:31:10.510648Z",
     "start_time": "2024-12-06T15:31:10.504988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Nombre d'images dans le dossier d'entraînement : {len(train_gen.image_list)}\")\n",
    "print(f\"Nombre de masques dans le dossier d'entraînement : {len(train_gen.mask_list)}\")\n"
   ],
   "id": "1d7946b841e63cfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'images dans le dossier d'entraînement : 2975\n",
      "Nombre de masques dans le dossier d'entraînement : 2975\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:31:15.901908Z",
     "start_time": "2024-12-06T15:31:13.991200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for images, masks in train_gen:\n",
    "    print(images.shape, masks.shape)\n",
    "    break\n"
   ],
   "id": "e8b4441ed0457e81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 256, 256, 3) (16, 256, 256, 1)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **3. Développement et entraînement du modèle U-Net:**",
   "id": "fbbe2a512296964c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Explication :** Cette cellule contient la définition et la construction du modèle U-Net. Le modèle est entraîné sur les données divisées avec 10 epochs. U-Net est bien adapté à la segmentation d'images. Des callbacks ont été ajoutés pour effectuer un early stopping et sauvegarder le meilleur modèle.\n",
   "id": "c39d75f151ddb0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T14:56:22.503032Z",
     "start_time": "2024-12-06T14:56:22.387106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fonction pour construire le modèle U-Net\n",
    "def unet_model(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Partie à la descente\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "\n",
    "    # Partie remontante\n",
    "    u4 = UpSampling2D((2, 2))(c3)\n",
    "    u4 = concatenate([u4, c2])\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(u4)\n",
    "\n",
    "    u5 = UpSampling2D((2, 2))(c4)\n",
    "    u5 = concatenate([u5, c1])\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(u5)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compilation du modèle\n",
    "model = unet_model()\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping et sauvegarde du meilleur modèle\n",
    "early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(os.path.join('models', 'unet_best_model.h5'), save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# Entraînement avec le générateur de données\n",
    "def train_unet_on_gpu():\n",
    "    with tf.device('/GPU:0'):  # Spécifie l'utilisation du GPU\n",
    "        model.fit(train_gen, validation_data=val_gen, epochs=10, callbacks=[early_stopping, model_checkpoint])\n"
   ],
   "id": "e9c1d850ba3cb54",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **4.  Comparaison des Modèles U-Net, VGG16-UNET, et U-Net Mini**",
   "id": "63bf1b5becc0baee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Explication :** Ici, nous avons ajouté un modèle U-Net Mini, qui est une version plus légère et simplifiée du U-Net. Le modèle VGG16-UNET a été mis à jour pour inclure une couche de data augmentation interne afin de mieux généraliser les performances.\n",
   "id": "4a679e3d2450a87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:08:41.715702Z",
     "start_time": "2024-12-06T15:08:40.558790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
    "\n",
    "# Développement du modèle VGG16-UNET\n",
    "def vgg16_unet_model(input_size=(256, 256, 3)):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_size)\n",
    "    \n",
    "    # Data augmentation intégrée\n",
    "    inputs = Input(input_size)\n",
    "    x = RandomFlip(\"horizontal\")(inputs)\n",
    "    x = RandomRotation(0.2)(x)\n",
    "    x = RandomZoom(0.2)(x)\n",
    "    \n",
    "    # Passage de l'entrée dans le modèle VGG16\n",
    "    block1_conv2 = base_model.get_layer('block1_conv2').output\n",
    "    block2_conv2 = base_model.get_layer('block2_conv2').output\n",
    "    block3_conv3 = base_model.get_layer('block3_conv3').output\n",
    "    vgg_outputs = [block1_conv2, block2_conv2, block3_conv3]\n",
    "    \n",
    "    # Définir un nouveau modèle avec les sorties intermédiaires désirées\n",
    "    vgg16_encoder = Model(inputs=base_model.input, outputs=vgg_outputs)\n",
    "    \n",
    "    # Passer les images par l'encodeur\n",
    "    c1, c2, c3 = vgg16_encoder(x)\n",
    "    \n",
    "    # Partie remontante du U-Net avec des couches de décodage\n",
    "    u4 = UpSampling2D((2, 2))(c3)\n",
    "    u4 = concatenate([u4, c2])\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(u4)\n",
    "    \n",
    "    u5 = UpSampling2D((2, 2))(c4)\n",
    "    u5 = concatenate([u5, c1])\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(u5)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    return model\n",
    "\n",
    "vgg16_model = vgg16_unet_model()\n",
    "vgg16_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle VGG16-UNET avec générateur de données\n",
    "vgg16_checkpoint = ModelCheckpoint(os.path.join('models', 'vgg16_best_model.h5'), save_best_only=True, monitor='val_loss')\n",
    "\n",
    "def train_vgg16_on_gpu():\n",
    "    with tf.device('/GPU:0'):  # Spécifie l'utilisation du GPU\n",
    "        vgg16_model.fit(train_gen, validation_data=val_gen, epochs=10, callbacks=[early_stopping, vgg16_checkpoint])\n",
    "\n"
   ],
   "id": "e63b342b2b51505d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Développement du modèle U-Net Mini\n",
    "def unet_mini_model(input_size=(256, 256, 3)):\n",
    "    inputs = Input(input_size)\n",
    "    c1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Partie à la descente\n",
    "    c3 = Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "\n",
    "    # Partie remontante\n",
    "    u4 = UpSampling2D((2, 2))(c3)\n",
    "    u4 = concatenate([u4, c2])\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same')(u4)\n",
    "\n",
    "    u5 = UpSampling2D((2, 2))(c4)\n",
    "    u5 = concatenate([u5, c1])\n",
    "    c5 = Conv2D(32, (3, 3), activation='relu', padding='same')(u5)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    return model\n",
    "\n",
    "unet_mini_model = unet_mini_model()\n",
    "unet_mini_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle U-Net Mini\n",
    "def train_unet_mini_on_gpu():\n",
    "    with tf.device('/GPU:0'):\n",
    "        unet_mini_model.fit(train_gen, validation_data=val_gen, epochs=10, callbacks=[early_stopping, unet_mini_checkpoint])"
   ],
   "id": "475e859f6c01d268"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Conclusion, Comparaison et Visualisation des Modèles",
   "id": "2c159dc8ff591131"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "** Cette cellule compare les trois modèles entraînés (U-Net classique, VGG16-UNET, et U-Net Mini) sur les métriques telles que l'accuracy, l'IoU, et le Dice Coefficient. Des graphiques sont créés pour visualiser les performances de chaque modèle. ",
   "id": "902b1c9cc27a68a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Comparaison des performances des trois modèles\n",
    "# On retient le modèle avec la meilleure précision sur l'ensemble de validation\n",
    "\n",
    "def compare_models(models, val_gen, test_gen):\n",
    "    metrics = []\n",
    "    for model in models:\n",
    "        model_name = model.name\n",
    "        loss, accuracy = model.evaluate(val_gen, verbose=0)\n",
    "        y_pred = model.predict(test_gen)\n",
    "        y_pred_binary = (y_pred > 0.5).astype(np.uint8)\n",
    "        y_test = np.concatenate([y for _, y in test_gen], axis=0)\n",
    "        \n",
    "        iou = jaccard_score(y_test.flatten(), y_pred_binary.flatten(), average='macro')\n",
    "        dice = (2 * np.sum(y_test.flatten() * y_pred_binary.flatten())) / (np.sum(y_test.flatten()) + np.sum(y_pred_binary.flatten()))\n",
    "        \n",
    "        metrics.append({\n",
    "            'model': model_name,\n",
    "            'loss': loss,\n",
    "            'accuracy': accuracy,\n",
    "            'iou': iou,\n",
    "            'dice': dice\n",
    "        })\n",
    "    return metrics\n",
    "\n",
    "# Comparer les modèles\n",
    "models = [model, vgg16_model, unet_mini_model]\n",
    "metrics = compare_models(models, val_gen, test_gen)"
   ],
   "id": "d164089490bec3aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualisation des résultats\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(metrics_df['model'], metrics_df['accuracy'], label='Accuracy', marker='o')\n",
    "plt.plot(metrics_df['model'], metrics_df['iou'], label='IoU', marker='o')\n",
    "plt.plot(metrics_df['model'], metrics_df['dice'], label='Dice Coefficient', marker='o')\n",
    "plt.title('Comparaison des métriques entre modèles')\n",
    "plt.xlabel('Modèles')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(metrics_df['model'], metrics_df['loss'], color='red')\n",
    "plt.title('Loss des différents modèles')\n",
    "plt.xlabel('Modèles')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ],
   "id": "74f61c9f0a7f5089"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Visualisation des exemples de résultats pour chaque modèle",
   "id": "6b639662d5040db4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fonction de visualisation des prédictions\n",
    "\n",
    "def visualize_predictions(models, test_gen, num_examples=3):\n",
    "    for model in models:\n",
    "        print(f\"Visualisation des prédictions pour le modèle: {model.name}\")\n",
    "        predictions = model.predict(test_gen)\n",
    "        X_test, y_test = next(iter(test_gen))\n",
    "        for i in range(num_examples):\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(X_test[i])\n",
    "            plt.title(\"Image d'origine\")\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(y_test[i].squeeze(), cmap='gray')\n",
    "            plt.title(\"Masque réel\")\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(predictions[i].squeeze(), cmap='gray')\n",
    "            plt.title(\"Masque prédit\")\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "# Visualiser quelques exemples de segmentation\n",
    "visualize_predictions(models, test_gen)"
   ],
   "id": "edd80304a6ea4d2b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
